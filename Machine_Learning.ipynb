{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project we will build a Machine Learning model to predict whether an indiviudal will have a stroke.  The data used in this project can be found on kaggle at the following link: https://www.kaggle.com/asaumya/healthcare-data#train_2v.csv\n",
    "\n",
    "# In this notebook, we build our Machine Learning model.  In our initial data analysis, we noticed that the individuals who had a stroke make up approximately 1.8% of the data.  We will use the Synthetic Minority Oversampling Technique (SMOTE) to account for this.\n",
    "\n",
    "# To view our initial data analysis, please see the notebook titled \"Data_Analysis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>age</th>\n",
       "      <th>average_glucose_level</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>employer_employed</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>58.0</td>\n",
       "      <td>87.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>employer_employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>110.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>employer_employed</td>\n",
       "      <td>smokes</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>employer_employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>161.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hypertension  heart_disease  ever_married          work_type  \\\n",
       "0             0              0             0           children   \n",
       "1             1              0             1  employer_employed   \n",
       "2             0              0             0  employer_employed   \n",
       "3             0              0             1  employer_employed   \n",
       "4             0              0             0  employer_employed   \n",
       "\n",
       "  smoking_status   age  average_glucose_level  stroke  \n",
       "0            NaN   3.0                  95.12       0  \n",
       "1   never smoked  58.0                  87.96       0  \n",
       "2            NaN   8.0                 110.89       0  \n",
       "3         smokes  70.0                  69.04       0  \n",
       "4            NaN  14.0                 161.28       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define file path to our data\n",
    "stoke_data_relevant_features_and_label_file_path = os.path.join(\"..\", \"Data\", \"stroke_data_relevant_features_and_label.csv\")\n",
    "\n",
    "# Create dataframe from local csv file \n",
    "stroke_data_relevant_features_and_label = pd.read_csv(stoke_data_relevant_features_and_label_file_path)\n",
    "\n",
    "# Previe dataframe\n",
    "stroke_data_relevant_features_and_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>average_glucose_level</th>\n",
       "      <th>stroke</th>\n",
       "      <th>hypertension_0</th>\n",
       "      <th>hypertension_1</th>\n",
       "      <th>heart_disease_0</th>\n",
       "      <th>heart_disease_1</th>\n",
       "      <th>ever_married_0</th>\n",
       "      <th>ever_married_1</th>\n",
       "      <th>work_type_Self-employed</th>\n",
       "      <th>work_type_children</th>\n",
       "      <th>work_type_employer_employed</th>\n",
       "      <th>smoking_status_formerly smoked</th>\n",
       "      <th>smoking_status_never smoked</th>\n",
       "      <th>smoking_status_smokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>95.12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>87.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>110.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>69.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>161.28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  average_glucose_level  stroke  hypertension_0  hypertension_1  \\\n",
       "0   3.0                  95.12       0               1               0   \n",
       "1  58.0                  87.96       0               0               1   \n",
       "2   8.0                 110.89       0               1               0   \n",
       "3  70.0                  69.04       0               1               0   \n",
       "4  14.0                 161.28       0               1               0   \n",
       "\n",
       "   heart_disease_0  heart_disease_1  ever_married_0  ever_married_1  \\\n",
       "0                1                0               1               0   \n",
       "1                1                0               0               1   \n",
       "2                1                0               1               0   \n",
       "3                1                0               0               1   \n",
       "4                1                0               1               0   \n",
       "\n",
       "   work_type_Self-employed  work_type_children  work_type_employer_employed  \\\n",
       "0                        0                   1                            0   \n",
       "1                        0                   0                            1   \n",
       "2                        0                   0                            1   \n",
       "3                        0                   0                            1   \n",
       "4                        0                   0                            1   \n",
       "\n",
       "   smoking_status_formerly smoked  smoking_status_never smoked  \\\n",
       "0                               0                            0   \n",
       "1                               0                            1   \n",
       "2                               0                            0   \n",
       "3                               0                            0   \n",
       "4                               0                            0   \n",
       "\n",
       "   smoking_status_smokes  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      1  \n",
       "4                      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform data to one hot encoded data\n",
    "machine_ready_stroke_data = pd.get_dummies(stroke_data_relevant_features_and_label, columns=[\"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \"smoking_status\"])\n",
    "machine_ready_stroke_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Maching Learning algorithm LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import other essential Machine Learning functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import SMOTE to handle the imbalanced data issue\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features and label\n",
    "X = np.array(machine_ready_stroke_data.drop([\"stroke\"], axis=1))\n",
    "y = np.array(machine_ready_stroke_data[\"stroke\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following section, we will run a for-loop to examine what order of SMOTE, split, scale (<em>SSS order</em>) yields the best results.  We will ignore any SSS order that scales before it splits, as this could bias the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      "\n",
      "1. SMOTE, split, scale\n",
      "Accuracy: 0.8131925433450659\n",
      "2. split, SMOTE, scale\n",
      "Accuracy: 0.8569892473118279\n",
      "3. split, scale, SMOTE\n",
      "Accuracy: 0.8841781874039939\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Iteration 2 \n",
      "\n",
      "1. SMOTE, split, scale\n",
      "Accuracy: 0.8091513492373875\n",
      "2. split, SMOTE, scale\n",
      "Accuracy: 0.8597542242703533\n",
      "3. split, scale, SMOTE\n",
      "Accuracy: 0.8841781874039939\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Iteration 3 \n",
      "\n",
      "1. SMOTE, split, scale\n",
      "Accuracy: 0.8146265154477904\n",
      "2. split, SMOTE, scale\n",
      "Accuracy: 0.8563748079877113\n",
      "3. split, scale, SMOTE\n",
      "Accuracy: 0.8811059907834101\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The most accurate order (highest average accuracy) is order 3, with an average accuracy of 0.8831541218637993\n",
      "The most stable order (lowest standard deviation) is order 3, with a standard deviation of 0.001773733545897503\n",
      "\n",
      "The least accurate order (least average accuracy) is order 1, with an average accuracy of 0.8123234693434146\n",
      "The least stable order (highest standard deviation) is order 1, with a standard deviation of 0.002839159818175792\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We want to determine which order of SMOTE, split, scale (SSS order) is best for this model\n",
    "# We will use the mean and stdev methods of the statistics library,\n",
    "# to find out which SSS order yields the highest average accuracy, and which order is the most stable (lowest standard deviation)\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Define variables holding the value for the each argument,\n",
    "# in order to easily change it in multiple places\n",
    "\n",
    "## SMOTE() parameters\n",
    "sampling_strategy_argument = 0.2\n",
    "k_neighbors_argument = 2\n",
    "\n",
    "## train_test_split() parameters\n",
    "test_size_argument = 0.15\n",
    "random_state_argument = 50\n",
    "\n",
    "## LogisticRegresssion() parameters\n",
    "solver_argument = \"liblinear\"\n",
    "C_argument = 0.00001\n",
    "\n",
    "# For every iteration in the loop,\n",
    "# we will append the accuracy of the current SSS order to it's own distinct list\n",
    "# After the loop has finished, we will calculate the average of each list\n",
    "# The list with the highest average we will call \"the most accuracte (on average)\"\n",
    "# we will also calculate the standard deviation of each list\n",
    "# The list with the lowest standard deviation we will call \"the most stable\"\n",
    "SSS_order_1_list = []\n",
    "SSS_order_2_list = []\n",
    "SSS_order_3_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    # Print the current iteration of the loop,\n",
    "    # in case we use a large number of iterations\n",
    "    print(f\"Iteration {i+1}\", \"\\n\")\n",
    "    \n",
    "    # Print the SSS order so we can analyze which one is \"best\"\n",
    "    print(\"1. SMOTE, split, scale\")\n",
    "        \n",
    "    # Use SMOTE to handle class imbalance\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy_argument, k_neighbors=k_neighbors_argument)\n",
    "    X_SMOTE, y_SMOTE = smote.fit_sample(X, y.ravel())\n",
    "    y_SMOTE = y_SMOTE.reshape(-1,1)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_SMOTE_train, X_SMOTE_test, y_SMOTE_train, y_SMOTE_test = train_test_split(X_SMOTE, y_SMOTE, test_size=test_size_argument, random_state=random_state_argument)\n",
    "    \n",
    "    # Create scaler for features\n",
    "    X_scaler = StandardScaler().fit(X_SMOTE_train)\n",
    "    \n",
    "    # Scale features\n",
    "    X_SMOTE_train_scaled = X_scaler.transform(X_SMOTE_train)\n",
    "    X_SMOTE_test_scaled = X_scaler.transform(X_SMOTE_test)\n",
    "    \n",
    "    # Create, fit, and score the Decision Tree Classifier\n",
    "    classifier = LogisticRegression(solver=solver_argument, C=C_argument)\n",
    "    classifier = classifier.fit(X=X_SMOTE_train_scaled, y=y_SMOTE_train)\n",
    "    score = classifier.score(X_SMOTE_test_scaled, y_SMOTE_test)\n",
    "    \n",
    "    # Append the score the the SSS_order_1 list,\n",
    "    # So that we can determine the average accuracy and standard deviation of SSS order 1\n",
    "    SSS_order_1_list.append(score)\n",
    "    \n",
    "    # Print the accuracy for the current iteration\n",
    "    print(f\"Accuracy: {score}\")\n",
    "    \n",
    "####################################################################################################\n",
    "    \n",
    "    # Print the SSS order so we can analyze which one is \"best\"\n",
    "    print(\"2. split, SMOTE, scale\")\n",
    "        \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_argument, random_state=random_state_argument)\n",
    "    \n",
    "    # Use SMOTE to handle class imbalance\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy_argument, k_neighbors=k_neighbors_argument)\n",
    "    X_train_SMOTE, y_train_SMOTE = smote.fit_sample(X_train, y_train.ravel())\n",
    "    y_train_SMOTE = y_train_SMOTE.reshape(-1,1)\n",
    "    \n",
    "    # Create scaler for features\n",
    "    X_SMOTE_scaler = StandardScaler().fit(X_train_SMOTE)\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_SMOTE_scaled = X_SMOTE_scaler.transform(X_train_SMOTE)\n",
    "    X_test_scaled = X_SMOTE_scaler.transform(X_test)\n",
    "    \n",
    "    # Create, fit, and score the Decision Tree Classifier\n",
    "    classifier = LogisticRegression(solver=solver_argument, C=C_argument)\n",
    "    classifier = classifier.fit(X=X_train_SMOTE_scaled, y=y_train_SMOTE)\n",
    "    score = classifier.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # Append the score the the SSS_order_2 list,\n",
    "    # So that we can determine the average accuracy and standard deviation of SSS order 2\n",
    "    SSS_order_2_list.append(score)\n",
    "    \n",
    "    # Print the accuracy for the current iteration\n",
    "    print(f\"Accuracy: {score}\")\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "    # Print the SSS order so we can analyze which one is \"best\"\n",
    "    print(\"3. split, scale, SMOTE\")\n",
    "        \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_argument, random_state=random_state_argument)\n",
    "    \n",
    "    # Create scaler for features\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Use SMOTE to handle class imbalance\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy_argument, k_neighbors=k_neighbors_argument)\n",
    "    X_train_scaled_SMOTE, y_train_SMOTE = smote.fit_sample(X_train_scaled, y_train.ravel())\n",
    "    y_train_SMOTE = y_train_SMOTE.reshape(-1,1)\n",
    "\n",
    "    # Create, fit, and score the Decision Tree Classifier\n",
    "    classifier = LogisticRegression(solver=solver_argument, C=C_argument)\n",
    "    classifier = classifier.fit(X=X_train_scaled_SMOTE, y=y_train_SMOTE)\n",
    "    score = classifier.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # Append the score the the SSS_order_3 list,\n",
    "    # So that we can determine the average accuracy and standard deviation of SSS order 3\n",
    "    SSS_order_3_list.append(score)\n",
    "    \n",
    "    # Print the accuracy for the current iteration\n",
    "    print(f\"Accuracy: {score}\")\n",
    "\n",
    "####################################################################################################\n",
    "    \n",
    "    # Print a long line with blank lines above and below,\n",
    "    # to easily see where one iteration of the loop ends, and the next starts\n",
    "    print()\n",
    "    print(100*\"-\")\n",
    "    print()\n",
    "    \n",
    "    # Increase the iterator by one\n",
    "    # so that the print statement at the beginning will show we're on the next iteration\n",
    "    i += 1\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Find the average accuracy of each SSS order,\n",
    "# and add each average to a list\n",
    "average_1 = mean(SSS_order_1_list)\n",
    "average_2 = mean(SSS_order_2_list)\n",
    "average_3 = mean(SSS_order_3_list)\n",
    "averages_list = [average_1, average_2, average_3]\n",
    "\n",
    "# Use conditionals to determine which SSS order has the highest average accuracy\n",
    "if max(averages_list) == averages_list[0]:\n",
    "    most_accurate_order = 1\n",
    "    average_accuracy_greatest = averages_list[0]\n",
    "    \n",
    "elif max(averages_list) == averages_list[1]:\n",
    "    most_accurate_order = 2\n",
    "    average_accuracy_greatest = averages_list[1]\n",
    "    \n",
    "elif max(averages_list) == averages_list[2]:\n",
    "    most_accurate_order = 3\n",
    "    average_accuracy_greatest = averages_list[2]\n",
    "\n",
    "# Print a message showing which SSS order has the highest average accuracy, along with it's accuracy\n",
    "print(f\"The most accurate order (highest average accuracy) is order {most_accurate_order}, with an average accuracy of {average_accuracy_greatest}\")\n",
    "        \n",
    "####################################################################################################\n",
    "\n",
    "# Find the standard deviation of the accuracy of each SSS order,\n",
    "# and add each standard deviation to a list\n",
    "standard_deviation_1 = stdev(SSS_order_1_list)\n",
    "standard_deviation_2 = stdev(SSS_order_2_list)\n",
    "standard_deviation_3 = stdev(SSS_order_3_list)\n",
    "standard_deviations_list = [standard_deviation_1, standard_deviation_2, standard_deviation_3]\n",
    "\n",
    "# Use conditionals to determine which SSS order has the lowest standard deviation\n",
    "if min(standard_deviations_list) == standard_deviations_list[0]:\n",
    "    most_stable_order = 1\n",
    "    lowest_standard_deviation = standard_deviations_list[0]\n",
    "    \n",
    "elif min(standard_deviations_list) == standard_deviations_list[1]:\n",
    "    most_stable_order = 2\n",
    "    lowest_standard_deviation = standard_deviations_list[1]\n",
    "    \n",
    "elif min(standard_deviations_list) == standard_deviations_list[2]:\n",
    "    most_stable_order = 3\n",
    "    lowest_standard_deviation = standard_deviations_list[2]\n",
    "    \n",
    "# Print a message showing which SSS order has the highest average accuracy, along with it's accuracy\n",
    "print(f\"The most stable order (lowest standard deviation) is order {most_stable_order}, with a standard deviation of {lowest_standard_deviation}\")\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# Print a blank line to separate the lines showing us the \"best\" orders from the lines showing us the \"worst\" orders\n",
    "print()\n",
    "\n",
    "# Use conditionals to determine which SSS order has the lowest average accuracy\n",
    "if min(averages_list) == averages_list[0]:\n",
    "    least_accurate_order = 1\n",
    "    average_accuracy_least = averages_list[0]\n",
    "    \n",
    "elif min(averages_list) == averages_list[1]:\n",
    "    least_accurate_order = 2\n",
    "    average_accuracy_least = averages_list[1]\n",
    "    \n",
    "elif min(averages_list) == averages_list[2]:\n",
    "    least_accurate_order = 3\n",
    "    average_accuracy_least = averages_list[2]\n",
    "\n",
    "# Print a message showing which SSS order has the highest average accuracy, along with it's accuracy\n",
    "print(f\"The least accurate order (least average accuracy) is order {least_accurate_order}, with an average accuracy of {average_accuracy_least}\")\n",
    "        \n",
    "####################################################################################################\n",
    "\n",
    "# Use conditionals to determine which SSS order has the highest standard deviation\n",
    "if max(standard_deviations_list) == standard_deviations_list[0]:\n",
    "    least_stable_order = 1\n",
    "    greatest_standard_deviation = standard_deviations_list[0]\n",
    "    \n",
    "elif max(standard_deviations_list) == standard_deviations_list[1]:\n",
    "    least_stable_order = 2\n",
    "    greatest_standard_deviation = standard_deviations_list[1]\n",
    "    \n",
    "elif max(standard_deviations_list) == standard_deviations_list[2]:\n",
    "    least_stable_order = 3\n",
    "    greatest_standard_deviation = standard_deviations_list[2]\n",
    "    \n",
    "# Print a message showing which SSS order has the highest average accuracy, along with it's accuracy\n",
    "print(f\"The least stable order (highest standard deviation) is order {least_stable_order}, with a standard deviation of {greatest_standard_deviation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time you run the loop above, you could get different results.  We notice that even though SSS order 3 often yields the highest average accuracy, it can also yield the least stability (highest standard deviation).   We also notice that even though SSS order 1 sometimes yields the most stability (lowest standard deviation), it can also yield the least average accuracy.  We therefore opt to employ the SSS order 2, or \"split, SMOTE, scale.\" This will sometimes yield the highest stability (lowest standard deviation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9086021505376344\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=50)\n",
    "    \n",
    "# Use SMOTE to handle class imbalance\n",
    "smote = SMOTE(sampling_strategy=0.2, k_neighbors=2)\n",
    "X_train_SMOTE, y_train_SMOTE = smote.fit_sample(X_train, y_train.ravel())\n",
    "y_train_SMOTE = y_train_SMOTE.reshape(-1,1)\n",
    "\n",
    "# Create scaler for features\n",
    "X_SMOTE_scaler = StandardScaler().fit(X_train_SMOTE)\n",
    "\n",
    "# Scale features\n",
    "X_train_SMOTE_scaled = X_SMOTE_scaler.transform(X_train_SMOTE)\n",
    "X_test_scaled = X_SMOTE_scaler.transform(X_test)\n",
    "\n",
    "# Create, fit, and score the Logistic Regression classifier\n",
    "# When we tested the Machine Learning model,\n",
    "# we saw that it gave too much weight to the \"age\" feature\n",
    "# No matter what value the other features had,\n",
    "# it seemed to return \"1\" if the age were above a certain value,\n",
    "# and \"0\" if the age were below a certain value\n",
    "# We therefore use a very low C value to stop the coefficient\n",
    "# of the age feature from becoming too large\n",
    "classifier = LogisticRegression(solver=\"liblinear\", penalty=\"l2\" , C=.00005)\n",
    "classifier = classifier.fit(X=X_train_SMOTE_scaled, y=y_train_SMOTE)\n",
    "score = classifier.score(X_test_scaled, y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16943796,  0.06752388, -0.05198894,  0.05198894, -0.07246565,\n",
       "         0.07246565, -0.0394961 ,  0.0394961 ,  0.04547211, -0.02821631,\n",
       "        -0.01745703, -0.00862261, -0.00256948,  0.02868548]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\web_development\\\\stroke_predictor.model']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export out final model and scaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "standard_scaler = StandardScaler().fit(X_train_SMOTE)\n",
    "\n",
    "standard_scaler_export_file_path = os.path.join(\"..\", \"web_development\", \"standard_scaler.model\")\n",
    "joblib.dump(standard_scaler, standard_scaler_export_file_path)\n",
    "\n",
    "classifier_export_file_path = os.path.join(\"..\", \"web_development\", \"stroke_predictor.model\")\n",
    "joblib.dump(classifier, classifier_export_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
